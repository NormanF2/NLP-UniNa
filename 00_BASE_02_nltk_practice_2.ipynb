{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **PRACTICE 2 - NLTK**\n",
    "Text pre-processing and normalization\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* BONUS\n",
    "* Exercise 2 - 16/03/2023\n",
    "\n",
    "Basic concepts reported from \"Speech and Language Processing (3rd ed. draft)\" by Dan Jurafsky and James H. Martin.\n",
    "\n",
    "### Tokenization\n",
    "The tokenization process is aimed at dividing strings into lists of substrings. For example, a tokenizer can be used to find the words and punctuation in a string. Case folding is another kind of normalization. Mapping everything to lower case means that Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many tasks, such as information retrieval or speech recognition.\n",
    "\n",
    "### Normalization\n",
    "Word normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh.\n",
    "\n",
    "### Lemmatization\n",
    "Is the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing.\n",
    "\n",
    "### Stemming\n",
    "Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.\"\n",
    "\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(f'Tokens: {tokens}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string\n",
    "normalized_tokens = [word.lower() for word in tokens]\n",
    "print(f'Normalized Tokens: {normalized_tokens}')\n",
    "punctuations = set(string.punctuation)\n",
    "punctuations.add('\\'')\n",
    "\n",
    "\n",
    "normalized_tokens = [w.translate(str.maketrans ('', '', string.punctuation)) for w in normalized_tokens]\n",
    "normalized_tokens = [w for w in normalized_tokens if len(w) >0]\n",
    "print(f'Normalized Tokens2: {normalized_tokens}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in normalized_tokens]\n",
    "print(f'Lemmatized tokens: {lemmatized_tokens}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming\n",
    "In the following we will use the Porter Stemmer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stemmed_words = [ps.stem(w) for w in normalized_tokens]\n",
    "print(f'Stemmed words {stemmed_words}')\n",
    "lemmed_stemmed_words = [ps.stem(w) for w in lemmatized_tokens]\n",
    "print(f'Stemmed words {stemmed_words}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BONUS - Stopword removal\n",
    "In the past it was common to remove high-frequency words from both\n",
    "the query and document before representing them. The list of such high-frequency\n",
    "stop list words to be removed is called a stop list. The intuition is that high-frequency terms\n",
    "(often function words like the, a, to) carry little semantic weight and may not help\n",
    "with retrieval."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.collocations as collocations\n",
    "from nltk.corpus import brown\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigrams_finder = collocations.BigramCollocationFinder.from_words(brown.words())\n",
    "bigrams_finder.apply_word_filter(lambda w: w.lower() in ignored_words)\n",
    "bigrams_finder.nbest(bigram_measures.pmi, 20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2 - 16/03/2023\n",
    "\n",
    "Repeat the Exercise 1 - 16/03/2023 reported \"23_March-16_nltk_practice_1\" but this time apply all the pre-processing and normalization techniques we covered. Compare the results obtained with and without these techniques."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
