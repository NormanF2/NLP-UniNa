{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "Y6boSNQtBELP"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **PRACTICE 1 - NLTK**\n",
    "\n",
    "*   Exploring a Corpus\n",
    "*   Vocabulary and Frequency distribution\n",
    "*   Collocations - Unigrams, Bigrams, Trigrams\n",
    "*   Exercise 1 - 16/03/2023"
   ],
   "metadata": {
    "id": "ontgkDNU-3XV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COLAB ONLY - Installing requirements"
   ],
   "metadata": {
    "id": "Y6boSNQtBELP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gr4Xo5L5-gdg",
    "outputId": "f9995f9a-044c-49eb-878c-87209f6efde6"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring a Corpus"
   ],
   "metadata": {
    "id": "-lbK0wiUBO3u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import nltk.corpus"
   ],
   "metadata": {
    "id": "MjoH2pmaAMmn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "50e190ec-3e1f-4b37-aeff-6f22901d6aa7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#A download utility to get a lot of data!\n",
    "nltk.download()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Getting the Brown corpus:\n",
    "from nltk.corpus import brown\n",
    "\n",
    "## Download a specific dataset\n",
    "try:\n",
    "    nltk.data.find('brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')\n",
    "\n",
    "## Check type and folder\n",
    "print(\"Brown corpora:\"+str(nltk.corpus.brown).replace('\\\\\\\\','/'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSZvoinfDHRz",
    "outputId": "a59638b7-5e45-4073-c963-a9ba4285d726"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Corpus categories\n",
    "print(f'{brown.categories()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Corpus files ids for a given category\n",
    "print(f'{brown.fileids(categories=\"adventure\")}')\n",
    "## Corpus files ids\n",
    "#print(f'{brown.fileids()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Getting first 30 word of first file\n",
    "n = 30\n",
    "category = 'adventure'\n",
    "fid = brown.fileids(categories=category)[0]\n",
    "\n",
    "print(f'{fid} first {n} words: {brown.words(fid)[:n]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Getting first sentence\n",
    "first_sentence = brown.sents(fid)[0]\n",
    "print(f'{fid} first {n} words: {first_sentence}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Getting first sentence as text\n",
    "first_sentence_text = ' '.join(first_sentence)\n",
    "print(f'{fid} text: {first_sentence_text}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'File \"{fid}\" sentences:')\n",
    "for i, s in enumerate(brown.sents(fileids=fid)[:10]):\n",
    "    print(f'{i} {\" \".join(s)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vocabulary and Frequency"
   ],
   "metadata": {
    "id": "8rgCOivPJujZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Vocabulary extraction and comparison\n",
    "vocabulary_fid = set(brown.words(fileids=fid))\n",
    "vocabulary_cat = set(brown.words(categories=category))\n",
    "\n",
    "print(f'File \"{fid}\" vocab size: {len(vocabulary_fid)}'\n",
    "      f'\\nCategory \"{category}\" vocab size: {len(vocabulary_cat)}')\n",
    "print(f'Brown tot words {len(brown.words())}')\n",
    "print(f'Brown tot words for {category} {len(brown.words(categories=category))}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3PUK-IQGORn",
    "outputId": "4287de95-94c5-427b-cd10-19f6ff65cdc3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Category vocabulary - Fid vocabulary:'\n",
    "      f'\\n-> size '\n",
    "      f'\\n--> cat - fid : {len(vocabulary_cat.difference(vocabulary_fid))}'\n",
    "      f'\\n--> fid - cat : {len(vocabulary_fid.difference(vocabulary_cat))} (Obviously!)'\n",
    "      #f'\\n-> words '\n",
    "      #f'\\n--> cat - fid : {vocabulary_cat.difference(vocabulary_fid)}'\n",
    "      #f'\\n--> fid - cat : {vocabulary_fid.difference(vocabulary_cat)}'\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocabulary_fid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "\n",
    "# Extracting frequency distribution for a specific file\n",
    "f_dist = FreqDist(brown.words(fileids=fid))\n",
    "f_dist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Looking for the 10 most frequent words\n",
    "f_dist.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why the dot (.) is the most common \"word\"?\n",
    "\n",
    "How do we solve this?"
   ],
   "metadata": {
    "id": "tKJ-41su5NqM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "punctuations = set(string.punctuation)\n",
    "punctuations.add('``')\n",
    "punctuations.add('\\'\\'')\n",
    "\n",
    "words = brown.words(fileids=fid)\n",
    "filtered_words = [w for w in words if w not in punctuations]\n",
    "fixed_f_dist = FreqDist(filtered_words)\n",
    "fixed_f_dist.most_common(20)"
   ],
   "metadata": {
    "id": "Xiry-3ar5Mmy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collocations - Unigrams, Bigrams\n",
    "the nltk.collocations gives you classes to extract Bigrams, Trigrams and so on.\n",
    "And Unigrams? know what? FreqDist gives you Unigrams :-)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk.collocations as collocations\n",
    "from nltk.corpus import brown\n",
    "import string\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "bigram_measures = collocations.BigramAssocMeasures()\n",
    "bigrams_finder = collocations.BigramCollocationFinder.from_words(brown.words())\n",
    "bigrams_finder.apply_word_filter(lambda w: w.lower() in punctuations)\n",
    "bigrams_finder.nbest(bigram_measures.pmi, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXERCISE 1 - 16/03/2023\n",
    "\n",
    "Using Brown's dataset \"Adventure\" category as your test set, compare it with the remaining part of the Brown dataset (namely all remaining categories together) and check:\n",
    "* Vocabulary size difference.\n",
    "* The intersection between the 100 most frequent words.\n",
    "* Compare the most common Bigrams, with different measures\n",
    "* Do the same with trigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
